{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beer Scraper\n",
    "\n",
    "6 Notebooks like this were uploaded to AWS EC2 instances in order to scrape the desired data and deposite said data in a MongoDB also hosted on AWS.\n",
    "\n",
    "As this is more-or-less a demo notebook or model of the actual, none of the code has been executed.\n",
    "\n",
    "I would also like to note that I ran this scraper at the beginning of 2018 so website structure could have changed since then make this code no longer work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Docker container I was using did not come with pymongo preinstalled.\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Necessary Imports\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connecting to MongoDatabase\n",
    "from pymongo import MongoClient\n",
    "# Actual IP and Ports changed for security reasons.\n",
    "client = MongoClient('250.250.250.250',20000)\n",
    "\n",
    "# setting path to collection\n",
    "Beers = client['Beers']\n",
    "beer_coll = Beers['Beer_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gathering all the beerstyles for a reference dictionary\n",
    "\n",
    "beer_styles_page = requests.get('https://www.beeradvocate.com/beer/style/')\n",
    "\n",
    "beer_styles_soup = BeautifulSoup(beer_styles_page.content,\"html5lib\")\n",
    "\n",
    "beer_styles = {}\n",
    "for beer in beer_styles_soup.find_all(name='tr'):\n",
    "    for style in beer.find_all('a'):\n",
    "        name = style.text\n",
    "        link = 'https://www.beeradvocate.com'+style['href']\n",
    "        beer_styles[name] = link "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Division by Style\n",
    "\n",
    "Once I ran the above and collected all the different styles and generated their associated landing pages, I randomly divided them up into 6 groups.  Each of the 6 scrapers got one of those groups. This example code has `scraper_group` as the variable containing an example set of beer styles to get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Quality Reviews Extractor Function Used on individual Beers.\n",
    "def quality_review_extractor(reviews_url, reviews_dict):\n",
    "    \n",
    "    # request page contents and convert to soup object\n",
    "    QR_beer_r = requests.get(reviews_url)\n",
    "    QR_beer_r_soup = BeautifulSoup(QR_beer_r.content, 'html5lib')\n",
    "    \n",
    "    # find all individual reviews\n",
    "    for ind_review in QR_beer_r_soup.find_all('div', attrs = {'id':'rating_fullview_container'}): \n",
    "        full_ind_review = ind_review.text # contains username and unwated numeric reviews\n",
    "\n",
    "        # unwanted attributes are contained in <span class=\"muted\">\"Unwated text</span>\n",
    "        numeric_review = ind_review.find_all(name ='span',attrs={'class':'muted'})\n",
    "    \n",
    "        # cleans out unwanted attributes (username, datetime, numeric ratings)\n",
    "        for reviewer in numeric_review:\n",
    "            unwanted_attr = reviewer.text\n",
    "            # replaces unwanted aspects\n",
    "            full_ind_review = full_ind_review.replace(unwanted_attr, '')\n",
    "            # scrubs out the 'Total Score' attribute\n",
    "            clean_review = re.sub( '^(.*)(%)',\"\",string = full_ind_review)\n",
    "        \n",
    "        # if the length of the remaining review is greater than 25 keep\n",
    "        if len(clean_review) > 25:\n",
    "            reviews_dict[str(len(reviews_dict)+1)] = clean_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Table row scraper\n",
    "def beer_table_scrape(table_row, current_type):\n",
    "    beer_tag = table_row.find_all('td')\n",
    "    # Try to get the general beer info from the row\n",
    "    try:\n",
    "        beer_name = beer_tag[0].text\n",
    "        brewery_name = beer_tag[1].text\n",
    "        abv = beer_tag[2].text\n",
    "        ratings = beer_tag[3].text\n",
    "\n",
    "        avg_score = beer_tag[4].text\n",
    "        beer_page = 'https://www.beeradvocate.com'+(beer_tag[0].find('a')['href']+'?sort=topr&start=')\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "        \n",
    "    quality_reviews = {} # empty dict to append reviews to\n",
    "    reviews_page = 0 \n",
    "    \n",
    "    \n",
    "    while len(quality_reviews) < 25 and reviews_page < 1000: # while we have less than 25 good reviews.  \n",
    "        reviews_page_url = beer_page+str(reviews_page) #url formula to change pages\n",
    "        \n",
    " \n",
    "        \n",
    "        print(\"Current Beer : \",beer_name,\n",
    "              \" --- Pages Scraped : \",int(reviews_page/25),\n",
    "              \" --- Quality Reviews : \",len(quality_reviews))\n",
    "        \n",
    "        print(reviews_page_url)\n",
    "        clear_output(wait=True) # clear print output so I dont stack print statements.  \n",
    "        \n",
    "        quality_review_extractor(reviews_page_url, quality_reviews) # hit the function\n",
    "        \n",
    "        quality_reviews['Beer_Name'] = beer_name\n",
    "        quality_reviews['Brewery_Name'] = brewery_name\n",
    "        quality_reviews['ABV'] = abv\n",
    "        quality_reviews['Type'] = current_type\n",
    "        \n",
    "        \n",
    "        reviews_page += 25 # value increment to change to next \n",
    "        \n",
    "        pause = np.random.lognormal(mean=1.5, sigma=0.4, size=1) #Lognormal dist, avg a 4.5sec pause.\n",
    "        \n",
    "        time.sleep(pause) #take a nap so beeradvocate does not get suspicious.  \n",
    "    \n",
    "    beer_coll.insert_one(quality_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraper_group = ['American Brown Ale', 'Belgian Pale Ale', 'Czech Pilsener', 'English Brown Ale',\n",
    "                 'Milk / Sweet Stout', 'Winter Warmer', 'English Stout', 'Flanders Oud Bruin', \n",
    "                 'Low Alcohol Beer', 'Gueuze', 'Belgian IPA', 'Irish Dry Stout', 'Kellerbier / Zwickelbier', \n",
    "                 'Foreign / Export Stout', 'Rauchbier', 'Faro', 'Bock', 'Belgian Strong Dark Ale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for each beer in sublist\n",
    "for individual_style in scraper_group: #random style list\n",
    "    base_beer_style_url = beer_styles[individual_style]+'?sort=revsD&start=' # base type url\n",
    "    page_count_value = 0\n",
    "    break_light = False\n",
    "    \n",
    "    #while we can still get access\n",
    "    while requests.get(base_beer_style_url+str(page_count_value)).status_code == 200 and break_light == False:\n",
    "        \n",
    "        # use individual style as our key to get the appropriate url & soup that shit\n",
    "        beer_style_table = BeautifulSoup(requests.get(base_beer_style_url+str(page_count_value)).content, 'html5lib')\n",
    "\n",
    "        # Loops through each row in the table on a page\n",
    "        for table_row in beer_style_table.find(name ='table', attrs = {'width':'100%'}).find_all('tr')[3:53]:\n",
    "            \n",
    "            if int(table_row.find_all('td')[3].text.replace(',','')) < 100:\n",
    "                break_light = True\n",
    "                break\n",
    "\n",
    "                \n",
    "            # perform that action, if it returns false, because we met the criteria\n",
    "            beer_table_scrape(table_row, individual_style)  #if the table scrape ends because of reviews being too low\n",
    "            \n",
    "                 # increment the page count\n",
    "        page_count_value += 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
